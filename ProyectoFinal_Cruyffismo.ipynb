{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_ZA8YXEHryK",
        "outputId": "69451a8d-1099-42e2-f71c-c659268a46dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VddGboysHte7",
        "outputId": "651f9813-1cbc-4949-8c03-fd9bcc6db845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1-O_N2YOeF3elR-Y6QYwBa1mK2MnxvzHb/TA_FINAL_TAC\n"
          ]
        }
      ],
      "source": [
        "#Modicar la carpeta donde están almacenadas las imágenes, anotaciones y base_checkpoint.pt\n",
        "%cd \"/content/drive/MyDrive/TA_FINAL_TAC/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tinl5vAHQiW"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_O--U-aEQa-",
        "outputId": "59db18dd-1e8f-41d5-bbdd-5bccafea5f03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f139d777a10>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision.transforms import functional as F_vision\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models import resnet50\n",
        "from torchvision.ops.boxes import box_area\n",
        "\n",
        "torch.random.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AmK9jVplhrVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prBbrPKIETOx"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import json\n",
        "import copy\n",
        "import shutil\n",
        "import random\n",
        "import requests\n",
        "from PIL import Image\n",
        "from os import listdir\n",
        "from os.path import exists\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = \"retina\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0FY2EKe7oFi"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-V3_JUOuADh"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n",
        "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\", normalize_before=False,\n",
        "                 return_intermediate_dec=False):\n",
        "      super().__init__()\n",
        "\n",
        "      encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
        "                                              dropout, activation, normalize_before)\n",
        "      encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "      self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "\n",
        "      decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
        "                                              dropout, activation, normalize_before)\n",
        "      decoder_norm = nn.LayerNorm(d_model)\n",
        "      self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n",
        "                                        return_intermediate=return_intermediate_dec)\n",
        "\n",
        "      self._reset_parameters()\n",
        "\n",
        "      self.d_model = d_model\n",
        "      self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "      for p in self.parameters():\n",
        "        if p.dim() > 1:\n",
        "          nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "      # flatten NxCxHxW a HWxNxC\n",
        "      bs, c, h, w = src.shape\n",
        "      src = src.flatten(2).permute(2, 0, 1)\n",
        "      pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "      query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "      mask = mask.flatten(1)\n",
        "\n",
        "      tgt = torch.zeros_like(query_embed)\n",
        "      memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "      hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,\n",
        "                        pos=pos_embed, query_pos=query_embed)\n",
        "      return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "      super().__init__()\n",
        "      self.layers = _get_clones(encoder_layer, num_layers)\n",
        "      self.num_layers = num_layers\n",
        "      self.norm = norm\n",
        "\n",
        "    def forward(self, src,\n",
        "                mask = None,\n",
        "                src_key_padding_mask = None,\n",
        "                pos = None):\n",
        "      output = src\n",
        "\n",
        "      for layer in self.layers:\n",
        "        output = layer(output, src_mask=mask,\n",
        "                        src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
        "\n",
        "      if self.norm is not None:\n",
        "        output = self.norm(output)\n",
        "\n",
        "      return output\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n",
        "      super().__init__()\n",
        "      self.layers = _get_clones(decoder_layer, num_layers)\n",
        "      self.num_layers = num_layers\n",
        "      self.norm = norm\n",
        "      self.return_intermediate = return_intermediate\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask = None,\n",
        "                memory_mask = None,\n",
        "                tgt_key_padding_mask = None,\n",
        "                memory_key_padding_mask = None,\n",
        "                pos = None,\n",
        "                query_pos = None):\n",
        "      output = tgt\n",
        "\n",
        "      intermediate = []\n",
        "\n",
        "      for layer in self.layers:\n",
        "        output = layer(output, memory, tgt_mask=tgt_mask,\n",
        "                        memory_mask=memory_mask,\n",
        "                        tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                        memory_key_padding_mask=memory_key_padding_mask,\n",
        "                        pos=pos, query_pos=query_pos)\n",
        "        if self.return_intermediate:\n",
        "          intermediate.append(self.norm(output))\n",
        "\n",
        "      if self.norm is not None:\n",
        "        output = self.norm(output)\n",
        "        if self.return_intermediate:\n",
        "          intermediate.pop()\n",
        "          intermediate.append(output)\n",
        "\n",
        "      if self.return_intermediate:\n",
        "        return torch.stack(intermediate)\n",
        "\n",
        "      return output.unsqueeze(0)\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\", normalize_before=False):\n",
        "      super().__init__()\n",
        "      self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "      # Implementacion del modelo Feedforward\n",
        "      self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "      self.norm1 = nn.LayerNorm(d_model)\n",
        "      self.norm2 = nn.LayerNorm(d_model)\n",
        "      self.dropout1 = nn.Dropout(dropout)\n",
        "      self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "      self.activation = _get_activation_fn(activation)\n",
        "      self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos):\n",
        "      return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     src,\n",
        "                     src_mask = None,\n",
        "                     src_key_padding_mask = None,\n",
        "                     pos = None):\n",
        "      q = k = self.with_pos_embed(src, pos)\n",
        "      src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n",
        "                            key_padding_mask=src_key_padding_mask)[0]\n",
        "      src = src + self.dropout1(src2)\n",
        "      src = self.norm1(src)\n",
        "      src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "      src = src + self.dropout2(src2)\n",
        "      src = self.norm2(src)\n",
        "      return src\n",
        "\n",
        "    def forward_pre(self, src,\n",
        "                    src_mask = None,\n",
        "                    src_key_padding_mask = None,\n",
        "                    pos = None):\n",
        "      src2 = self.norm1(src)\n",
        "      q = k = self.with_pos_embed(src2, pos)\n",
        "      src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n",
        "                            key_padding_mask=src_key_padding_mask)[0]\n",
        "      src = src + self.dropout1(src2)\n",
        "      src2 = self.norm2(src)\n",
        "      src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
        "      src = src + self.dropout2(src2)\n",
        "      return src\n",
        "\n",
        "    def forward(self, src,\n",
        "                src_mask = None,\n",
        "                src_key_padding_mask = None,\n",
        "                pos = None):\n",
        "      if self.normalize_before:\n",
        "        return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
        "      return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\", normalize_before=False):\n",
        "      super().__init__()\n",
        "      self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "      self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "      # Implementacion del modelo Feedforward\n",
        "      self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "      self.norm1 = nn.LayerNorm(d_model)\n",
        "      self.norm2 = nn.LayerNorm(d_model)\n",
        "      self.norm3 = nn.LayerNorm(d_model)\n",
        "      self.dropout1 = nn.Dropout(dropout)\n",
        "      self.dropout2 = nn.Dropout(dropout)\n",
        "      self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "      self.activation = _get_activation_fn(activation)\n",
        "      self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos):\n",
        "      return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self, tgt, memory,\n",
        "                     tgt_mask = None,\n",
        "                     memory_mask = None,\n",
        "                     tgt_key_padding_mask = None,\n",
        "                     memory_key_padding_mask = None,\n",
        "                     pos = None,\n",
        "                     query_pos = None):\n",
        "      q = k = self.with_pos_embed(tgt, query_pos)\n",
        "      tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n",
        "                            key_padding_mask=tgt_key_padding_mask)[0]\n",
        "      tgt = tgt + self.dropout1(tgt2)\n",
        "      tgt = self.norm1(tgt)\n",
        "      tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                  key=self.with_pos_embed(memory, pos),\n",
        "                                  value=memory, attn_mask=memory_mask,\n",
        "                                  key_padding_mask=memory_key_padding_mask)[0]\n",
        "      tgt = tgt + self.dropout2(tgt2)\n",
        "      tgt = self.norm2(tgt)\n",
        "      tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "      tgt = tgt + self.dropout3(tgt2)\n",
        "      tgt = self.norm3(tgt)\n",
        "      return tgt\n",
        "\n",
        "    def forward_pre(self, tgt, memory,\n",
        "                    tgt_mask = None,\n",
        "                    memory_mask = None,\n",
        "                    tgt_key_padding_mask = None,\n",
        "                    memory_key_padding_mask = None,\n",
        "                    pos = None,\n",
        "                    query_pos = None):\n",
        "      tgt2 = self.norm1(tgt)\n",
        "      q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "      tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n",
        "                            key_padding_mask=tgt_key_padding_mask)[0]\n",
        "      tgt = tgt + self.dropout1(tgt2)\n",
        "      tgt2 = self.norm2(tgt)\n",
        "      tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
        "                                  key=self.with_pos_embed(memory, pos),\n",
        "                                  value=memory, attn_mask=memory_mask,\n",
        "                                  key_padding_mask=memory_key_padding_mask)[0]\n",
        "      tgt = tgt + self.dropout2(tgt2)\n",
        "      tgt2 = self.norm3(tgt)\n",
        "      tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
        "      tgt = tgt + self.dropout3(tgt2)\n",
        "      return tgt\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask = None,\n",
        "                memory_mask = None,\n",
        "                tgt_key_padding_mask = None,\n",
        "                memory_key_padding_mask = None,\n",
        "                pos = None,\n",
        "                query_pos = None):\n",
        "      if self.normalize_before:\n",
        "        return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
        "                                tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
        "      return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
        "                                tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "  \"\"\"Retorno de una función de activación a partir de una cadena\"\"\"\n",
        "  if activation == \"relu\":\n",
        "    return F.relu\n",
        "  if activation == \"gelu\":\n",
        "    return F.gelu\n",
        "  if activation == \"glu\":\n",
        "    return F.glu\n",
        "  raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJq2eft5tuzX"
      },
      "outputs": [],
      "source": [
        "class PositionEmbeddingSine(nn.Module):\n",
        "  def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_pos_feats = num_pos_feats\n",
        "    self.temperature = temperature\n",
        "    self.normalize = normalize\n",
        "    \n",
        "    if (scale is not None) and (normalize is False):\n",
        "      raise ValueError(\"normalize should be True if scale is passed\")\n",
        "    \n",
        "    if scale is None:\n",
        "      scale = 2 * math.pi\n",
        "    \n",
        "    self.scale = scale\n",
        "\n",
        "  def forward(self, x, masks):\n",
        "    assert masks is not None\n",
        "\n",
        "    not_masks = ~masks\n",
        "    \n",
        "    y_embed = not_masks.cumsum(1, dtype=torch.float32)\n",
        "    x_embed = not_masks.cumsum(2, dtype=torch.float32)\n",
        "    \n",
        "    if self.normalize:\n",
        "      eps = 1e-6\n",
        "      y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "      x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "\n",
        "    dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
        "    dim_t = self.temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"trunc\") / self.num_pos_feats)\n",
        "\n",
        "    pos_x = x_embed[:, :, :, None] / dim_t\n",
        "    pos_y = y_embed[:, :, :, None] / dim_t\n",
        "    \n",
        "    pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "    pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "    \n",
        "    pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "    \n",
        "    return pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqoX7h5JtuzY"
      },
      "outputs": [],
      "source": [
        "class DETR(nn.Module):\n",
        "  def __init__(self, num_classes, num_queries=100, hidden_dim=256, embedding_size=50, nheads=8, num_encoder_layers=6, num_decoder_layers=6):\n",
        "    super().__init__()\n",
        "\n",
        "    #Creación de ResNet-50 backbone\n",
        "    self.backbone = resnet50(pretrained=True)\n",
        "\n",
        "    #Borrada de las últimas capas de ResNet-50\n",
        "    del self.backbone.fc\n",
        "    del self.backbone.avgpool\n",
        "\n",
        "    #Codificaciones posicionales espaciales, el embedding_size depende de la salida de la red\n",
        "    #Por ejemplo:\n",
        "    #  inputs: (batch_size, 3, 800, 800) -> backbone's output: (batch_size, 2048, 25, 25)\n",
        "    #  inputs: (batch_size, 3, 1600, 1600) -> backbone's output: (batch_size, 2048, 50, 50)\n",
        "    n_steps = hidden_dim // 2\n",
        "\n",
        "    self.position_embedding = PositionEmbeddingSine(num_pos_feats=n_steps, normalize=True)\n",
        "\n",
        "    #Cración de capa conv\n",
        "    self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
        "\n",
        "    #Codificaciones posicionales de salida\n",
        "    self.query_pos = nn.Embedding(num_queries, hidden_dim)\n",
        "\n",
        "    #Creación de transformer\n",
        "    self.transformer = Transformer(hidden_dim, nheads, num_encoder_layers, num_decoder_layers, normalize_before=False, return_intermediate_dec=True)\n",
        "\n",
        "    #Capa de Regresión\n",
        "    self.linear_1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.linear_2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
        "    \n",
        "    #Capa de clasificación - Una clase extra para predecir los espacios vacíos\n",
        "    self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
        "  \n",
        "  def forward(self, inputs_and_masks):\n",
        "    inputs = inputs_and_masks[0]\n",
        "    masks = inputs_and_masks[1]\n",
        "\n",
        "    #Inputs de propagación (batch_size, 3, height, width) para ResNet-50\n",
        "    x = self.backbone.conv1(inputs)\n",
        "    x = self.backbone.bn1(x)\n",
        "    x = self.backbone.relu(x)\n",
        "    x = self.backbone.maxpool(x)\n",
        "\n",
        "    x = self.backbone.layer1(x)\n",
        "    x = self.backbone.layer2(x)\n",
        "    x = self.backbone.layer3(x)\n",
        "    x = self.backbone.layer4(x)\n",
        "\n",
        "    #Adaptación de las formas de la máscara\n",
        "    new_masks_height_and_width = x.shape[2:]\n",
        "\n",
        "    masks = F.interpolate(masks[None].float(), size=new_masks_height_and_width).to(torch.bool)[0]\n",
        "\n",
        "    #Se obtienen codificaciones posicionales espaciales\n",
        "    pos = self.position_embedding(x, masks)\n",
        "    \n",
        "    #Se convierte de 2048 a 256 planos característicos para el transformer\n",
        "    x = self.conv(x)\n",
        "\n",
        "    #Se propaga a través del transformer\n",
        "    x = self.transformer(x, masks, self.query_pos.weight, pos)[0]\n",
        "\n",
        "    #La salida permite obtener la clasificación y los bounding boxes\n",
        "    pred_logits = self.linear_class(x)\n",
        "    pred_logits = pred_logits[-1]\n",
        "\n",
        "    x = self.linear_1(x)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = self.linear_2(x)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    pred_bboxes = self.linear_bbox(x)\n",
        "    pred_bboxes = pred_bboxes.sigmoid()\n",
        "    pred_bboxes = pred_bboxes[-1]\n",
        "\n",
        "    return {\"pred_logits\": pred_logits, \"pred_bboxes\": pred_bboxes}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mw64VBnivsi"
      },
      "source": [
        "## Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw2WRN9MtuzY"
      },
      "outputs": [],
      "source": [
        "def create_model(num_classes):\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  model = DETR(num_classes)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GafV0kqtuzY",
        "outputId": "c5c0b607-dd31-41a0-83e6-b52f03628c33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "device_condition = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "device = torch.device(device_condition)\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "77464af022044613bec8498f5cfd389b",
            "daab1cd759014010ba69852f739eaa44",
            "326a4b882cf74509bcc2e7a389aa614b",
            "624858025d6544a7b8b2b9bd9e952e99",
            "d44392fee29a40a6a065f952192adc21",
            "e507fe71f9d54a2c8879a162c62a419b",
            "58e903873085427c8673513983603787",
            "2e7263e47ec74a4783ad90fb4203fc88",
            "c7d16c291a3541928c2c1d275416d526",
            "65f8eda2fd2e4a9cb55c9b5b629f9df4",
            "a6f9d60c3e834077a53f6266c2caa4a6"
          ]
        },
        "id": "h-U9z6OlHYwq",
        "outputId": "2757adb0-d592-4e19-e6c7-3c74e55559ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77464af022044613bec8498f5cfd389b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Modificar el número de clases dependiendo del dataset\n",
        "num_classes = 6\n",
        "\n",
        "model = create_model(num_classes=num_classes)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7615Hkbqq1y"
      },
      "source": [
        "## Functions for Load Model without the Classification Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMN7GeErsArk"
      },
      "outputs": [],
      "source": [
        "def load_pre_trained_model_weights(model, model_file):\n",
        "  model.load_state_dict(torch.load(model_file), strict=False)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxlywdfqsKtO"
      },
      "outputs": [],
      "source": [
        "model_file = \"base_checkpoint.pt\"\n",
        "\n",
        "model = load_pre_trained_model_weights(model, model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGOaAQNorcq9"
      },
      "outputs": [],
      "source": [
        "for parameter in model.backbone.parameters():\n",
        "  parameter.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttA7dW-A8Bp1"
      },
      "source": [
        "## Functions for Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIOxdwnVRuAC"
      },
      "outputs": [],
      "source": [
        "def save_weights(model, optimizer, lr_scheduler, model_file, optimizer_file, lr_scheduler_file):\n",
        "  torch.save(model.state_dict(), model_file)\n",
        "  torch.save(optimizer.state_dict(), optimizer_file)\n",
        "  torch.save(lr_scheduler.state_dict(), lr_scheduler_file)\n",
        "\n",
        "def reset_weights(model, optimizer, lr_scheduler, model_file, optimizer_file, lr_scheduler_file):\n",
        "  model.load_state_dict(torch.load(model_file))\n",
        "\n",
        "  optimizer.load_state_dict(torch.load(optimizer_file))\n",
        "\n",
        "  lr_scheduler.load_state_dict(torch.load(lr_scheduler_file))\n",
        "\n",
        "  return model, optimizer, lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ_8gGtEpVUX"
      },
      "outputs": [],
      "source": [
        "def crop(image, target, region):\n",
        "    cropped_image = F_vision.crop(image, *region)\n",
        "\n",
        "    target = target.copy()\n",
        "    i, j, h, w = region\n",
        "\n",
        "    fields = [\"labels\"]\n",
        "\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        max_size = torch.as_tensor([w, h], dtype=torch.float32)\n",
        "        cropped_boxes = boxes - torch.as_tensor([j, i, j, i])\n",
        "        cropped_boxes = torch.min(cropped_boxes.reshape(-1, 2, 2), max_size)\n",
        "        cropped_boxes = cropped_boxes.clamp(min=0)\n",
        "        area = (cropped_boxes[:, 1, :] - cropped_boxes[:, 0, :]).prod(dim=1)\n",
        "        target[\"boxes\"] = cropped_boxes.reshape(-1, 4)\n",
        "        fields.append(\"boxes\")\n",
        "\n",
        "    # Se remueven elementos para los que el box tiene 0 de área\n",
        "    if \"boxes\" in target:\n",
        "        # Favorecer la selección de cuadros al definir qué elementos conservar\n",
        "        cropped_boxes = target['boxes'].reshape(-1, 2, 2)\n",
        "        keep = torch.all(cropped_boxes[:, 1, :] > cropped_boxes[:, 0, :], dim=1)\n",
        "\n",
        "        target[\"labels\"] = target[\"labels\"][keep]\n",
        "        target[\"boxes\"] = target[\"boxes\"][keep]\n",
        "\n",
        "    return cropped_image, target\n",
        "\n",
        "def resize(image, target, size, max_size=None):\n",
        "    # el tamaño puede ser min_size (escalar) o (w, h) tupla\n",
        "\n",
        "    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n",
        "        w, h = image_size\n",
        "        if max_size is not None:\n",
        "            min_original_size = float(min((w, h)))\n",
        "            max_original_size = float(max((w, h)))\n",
        "            if max_original_size / min_original_size * size > max_size:\n",
        "                size = int(round(max_size * min_original_size / max_original_size))\n",
        "\n",
        "        if (w <= h and w == size) or (h <= w and h == size):\n",
        "            return (h, w)\n",
        "\n",
        "        if w < h:\n",
        "            ow = size\n",
        "            oh = int(size * h / w)\n",
        "        else:\n",
        "            oh = size\n",
        "            ow = int(size * w / h)\n",
        "\n",
        "        return (oh, ow)\n",
        "\n",
        "    def get_size(image_size, size, max_size=None):\n",
        "        if isinstance(size, (list, tuple)):\n",
        "            return size[::-1]\n",
        "        else:\n",
        "            return get_size_with_aspect_ratio(image_size, size, max_size)\n",
        "\n",
        "    size = get_size(image.size, size, max_size)\n",
        "    rescaled_image = F_vision.resize(image, size)\n",
        "\n",
        "    if target is None:\n",
        "        return rescaled_image, None\n",
        "\n",
        "    ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(rescaled_image.size, image.size))\n",
        "    ratio_width, ratio_height = ratios\n",
        "\n",
        "    target = target.copy()\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        scaled_boxes = boxes * torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height])\n",
        "        target[\"boxes\"] = scaled_boxes\n",
        "\n",
        "    return rescaled_image, target\n",
        "\n",
        "def hflip(image, target):\n",
        "    flipped_image = F_vision.hflip(image)\n",
        "\n",
        "    w, h = image.size\n",
        "\n",
        "    target = target.copy()\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        boxes = boxes[:, [2, 1, 0, 3]] * torch.as_tensor([-1, 1, -1, 1]) + torch.as_tensor([w, 0, w, 0])\n",
        "        target[\"boxes\"] = boxes\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = target['masks'].flip(-1)\n",
        "\n",
        "    return flipped_image, target\n",
        "\n",
        "def bbox_xyxy_to_cxcywh(x):\n",
        "  #Convertir x_0, y_0, x_1, y_1 a x_c, y_c, w, h para un bounding box (x_0, y_0), (x_0, y_1), (x_1, y_1), (x_1, y_0) donde x_c, y_c son x_center, y_center\n",
        "  x0, y0, x1, y1 = x.unbind(-1)\n",
        "  b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
        "       (x1 - x0), (y1 - y0)]\n",
        "  return torch.stack(b, dim=-1)\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, target=None):\n",
        "        image = F_vision.normalize(image, mean=self.mean, std=self.std)\n",
        "        if target is None:\n",
        "            return image, None\n",
        "        target = target.copy()\n",
        "        h, w = image.shape[-2:]\n",
        "        if \"boxes\" in target:\n",
        "            boxes = target[\"boxes\"]\n",
        "            boxes = bbox_xyxy_to_cxcywh(boxes)\n",
        "            boxes = boxes / torch.tensor([w, h, w, h], dtype=torch.float32)\n",
        "            target[\"boxes\"] = boxes\n",
        "        return image, target\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, img, target=None):\n",
        "        return F_vision.to_tensor(img), target\n",
        "\n",
        "class RandomSizeCrop(object):\n",
        "    def __init__(self, min_size: int, max_size: int):\n",
        "        self.min_size = min_size\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        w = random.randint(self.min_size, min(img.width, self.max_size))\n",
        "        h = random.randint(self.min_size, min(img.height, self.max_size))\n",
        "        region = T.RandomCrop.get_params(img, [h, w])\n",
        "        return crop(img, target, region)\n",
        "\n",
        "class RandomResize(object):\n",
        "    def __init__(self, sizes, max_size=None):\n",
        "        assert isinstance(sizes, (list, tuple))\n",
        "        self.sizes = sizes\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __call__(self, img, target=None):\n",
        "        size = random.choice(self.sizes)\n",
        "        return resize(img, target, size, self.max_size)\n",
        "\n",
        "class RandomSelect(object):\n",
        "    \"\"\"\n",
        "    Selecciona aleatoriamente entre transforms1 y transforms2,\n",
        "    con probabilidad p para transforms1 y (1 - p) para transforms2\n",
        "    \"\"\"\n",
        "    def __init__(self, transforms1, transforms2, p=0.5):\n",
        "        self.transforms1 = transforms1\n",
        "        self.transforms2 = transforms2\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        if random.random() < self.p:\n",
        "            return self.transforms1(img, target)\n",
        "        return self.transforms2(img, target)\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        if random.random() < self.p:\n",
        "            return hflip(img, target)\n",
        "        return img, target\n",
        "\n",
        "class TargetToTensor(object):\n",
        "    def __call__(self, img, target):\n",
        "        if target != None:\n",
        "            target[\"labels\"] = torch.tensor(target[\"labels\"])\n",
        "            target[\"boxes\"] = torch.tensor(target[\"boxes\"])\n",
        "\n",
        "        return img, target\n",
        "\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target=None):\n",
        "        for t in self.transforms:\n",
        "            transform_result = t(image, target)\n",
        "            if type(transform_result) == tuple:\n",
        "              image, target = transform_result\n",
        "            else:\n",
        "              image, target = transform_result, None\n",
        "        if target is None:\n",
        "          return image\n",
        "        return image, target\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + \"(\"\n",
        "        for t in self.transforms:\n",
        "            format_string += \"\\n\"\n",
        "            format_string += \"    {0}\".format(t)\n",
        "        format_string += \"\\n)\"\n",
        "        return format_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOrx7YdYtuzZ"
      },
      "outputs": [],
      "source": [
        "def get_tensors_shape(tensor_list):\n",
        "  tensor_shapes_list = [list(tensor.shape) for tensor in tensor_list]\n",
        "\n",
        "  max_tensor_shape = tensor_shapes_list[0]\n",
        "\n",
        "  for tensor_shape in tensor_shapes_list[1:]:\n",
        "    max_tensor_shape = [max(first_value, second_value) for first_value, second_value in zip(max_tensor_shape, tensor_shape)]\n",
        "  \n",
        "  tensors_shape = [len(tensor_list)] + max_tensor_shape\n",
        "\n",
        "  return tensors_shape\n",
        "\n",
        "def padding_tensors_and_generate_masks(tensor_list, return_masks):\n",
        "  dtype = tensor_list[0].dtype\n",
        "  device = tensor_list[0].device\n",
        "\n",
        "  tensors_shape = get_tensors_shape(tensor_list)\n",
        "\n",
        "  batch_size, channels, height, width = tensors_shape\n",
        "\n",
        "  tensors_padded = torch.zeros((batch_size, channels, height, width), dtype=dtype, device=device)\n",
        "  \n",
        "  if return_masks == True:\n",
        "    masks = torch.ones((batch_size, height, width), dtype=torch.bool, device=device)\n",
        "\n",
        "    for original_tensor, tensor_padded, mask in zip(tensor_list, tensors_padded, masks):\n",
        "      tensor_padded[:original_tensor.shape[0], :original_tensor.shape[1], :original_tensor.shape[2]] = original_tensor\n",
        "      mask[:original_tensor.shape[1], :original_tensor.shape[2]] = False\n",
        "  else:\n",
        "    for original_tensor, tensor_padded in zip(tensor_list, tensors_padded):\n",
        "      tensor_padded[:original_tensor.shape[0], :original_tensor.shape[1], :original_tensor.shape[2]] = original_tensor\n",
        "\n",
        "  if return_masks == True:\n",
        "    return tensors_padded, masks\n",
        "  elif return_masks == False:\n",
        "    return tensors_padded\n",
        "\n",
        "def get_transformation(mode=\"train\"):\n",
        "  scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
        "\n",
        "  random_select = RandomSelect(\n",
        "                                  RandomResize(scales, max_size=1333),\n",
        "                                  Compose([\n",
        "                                            RandomResize([400, 500, 600]),\n",
        "                                            RandomSizeCrop(384, 600),\n",
        "                                            RandomResize(scales, max_size=1333),\n",
        "                                          ])\n",
        "                              )\n",
        "  \n",
        "  normalize = Compose([\n",
        "                        ToTensor(),\n",
        "                        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                       ])\n",
        "\n",
        "  if mode == \"train\":\n",
        "    return Compose([\n",
        "                      TargetToTensor(),\n",
        "                      RandomHorizontalFlip(),\n",
        "                      random_select,\n",
        "                      normalize,\n",
        "                    ])\n",
        "  elif mode == \"eval\":\n",
        "    return Compose([\n",
        "                      TargetToTensor(),\n",
        "                      RandomResize([800], max_size=1333),\n",
        "                      normalize,\n",
        "                    ])\n",
        "  else:\n",
        "    raise ValueError(f\"Unknown mode ({mode})\")\n",
        "\n",
        "def process_images(images, mode, device, return_masks=False):\n",
        "  transform = get_transformation(mode=mode)\n",
        "  \n",
        "  #Mean-Std normaliza las imágenes de entrada\n",
        "  images = [transform(image) for image in images]\n",
        "\n",
        "  if return_masks == True:\n",
        "    images, masks = padding_tensors_and_generate_masks(images, return_masks)\n",
        "\n",
        "    return images.to(device), masks.to(device)\n",
        "  else:\n",
        "    images = padding_tensors_and_generate_masks(images, return_masks)\n",
        "\n",
        "    return images.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqQSLqrGFRRk"
      },
      "outputs": [],
      "source": [
        "class ApplyTransformationToDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, images_dataset, annotations_dataset, transform):\n",
        "    self.images_dataset = images_dataset\n",
        "    self.annotations_dataset = annotations_dataset\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_path = self.images_dataset[index]\n",
        "    annotation_path = self.annotations_dataset[index]\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    \n",
        "    with open(annotation_path, \"r\") as json_file:\n",
        "      target = json.load(json_file)\n",
        "\n",
        "    image, target = self.transform(image, target)\n",
        "\n",
        "    return image, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a28PnG0ysNMF"
      },
      "outputs": [],
      "source": [
        "def padding_sample_batch(sample_batch):\n",
        "  images, targets = zip(*sample_batch)\n",
        "  \n",
        "  images, masks = padding_tensors_and_generate_masks(images, True)\n",
        "  \n",
        "  return {\"images\": images, \"masks\": masks}, [target for target in targets]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLpOWuV5uo_V"
      },
      "outputs": [],
      "source": [
        "def box_iou(boxes1, boxes2):\n",
        "    area1 = box_area(boxes1)\n",
        "    area2 = box_area(boxes2)\n",
        "\n",
        "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
        "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
        "\n",
        "    #Sujeta todos los elementos en la entrada en el rango [min, max]. Si min es Ninguno, no hay límite inferior. O bien, si max es Ninguno, no hay límite superior.\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
        "\n",
        "    union = area1[:, None] + area2 - inter\n",
        "\n",
        "    iou = inter / union\n",
        "    return iou, union\n",
        "\n",
        "\n",
        "def generalized_box_iou(boxes1, boxes2):\n",
        "    # Los boxes degenerados dan como resultados inf/nan por lo tanto se debe realizar una revisión\n",
        "    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
        "    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
        "    iou, union = box_iou(boxes1, boxes2)\n",
        "\n",
        "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
        "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    area = wh[:, :, 0] * wh[:, :, 1]\n",
        "\n",
        "    return iou - (area - union) / area\n",
        "\n",
        "def matching_function_for_bounding_boxes(predictions, targets):\n",
        "  predictions = predictions.flatten(0, 1) # [batch_size * num_queries, 4]\n",
        "  targets = torch.cat(targets)\n",
        "\n",
        "  #p value para la distancia p-norm\n",
        "  p_value_for_p_norm = 1\n",
        "  \n",
        "  #Calcula la distancia p-norm entre los boxes.\n",
        "  cost_bbox = torch.cdist(predictions, targets, p=p_value_for_p_norm)\n",
        "\n",
        "  #Calcula el costo GIoU (Generalized IoU) entre los boxes\n",
        "  cost_giou = -generalized_box_iou(bbox_cxcywh_to_xyxy(predictions), bbox_cxcywh_to_xyxy(targets))\n",
        "\n",
        "  return cost_bbox, cost_giou\n",
        "\n",
        "def matching_function_for_classification(predictions, targets):\n",
        "  predictions = predictions.flatten(0, 1) # [batch_size * num_queries, num_classes]\n",
        "  targets = torch.cat(targets)\n",
        "\n",
        "  # Calcula el costo de clasificación. Al contrario que el loss, no se usa el loss de probabilidad logarítmica negativa\n",
        "  # pero aproximado en 1 - probabilidades[clase objetivo]. El 1 es una constante que no cambia el match,\n",
        "  # por lo que puede ser omitido.\n",
        "  cost_classification = -predictions[:, targets]\n",
        "\n",
        "  return cost_classification\n",
        "\n",
        "def matching_function(prediction_logits, prediction_bboxes, target_labels, target_bboxes, cost_for_classification=1, cost_for_bbox=5, cost_for_giou=2):\n",
        "  prediction_probabilities = prediction_logits.softmax(-1)\n",
        "\n",
        "  batch_size, num_queries, _ = prediction_probabilities.shape\n",
        "\n",
        "  cost_classification = matching_function_for_classification(prediction_probabilities, target_labels)\n",
        "\n",
        "  cost_bbox, cost_giou = matching_function_for_bounding_boxes(prediction_bboxes, target_bboxes)\n",
        "\n",
        "  # Calcula el costo de la matriz\n",
        "  total_cost = (cost_for_classification * cost_classification) + (cost_for_bbox * cost_bbox) + (cost_for_giou * cost_giou)\n",
        "  \n",
        "  total_cost = total_cost.view(batch_size, num_queries, -1)\n",
        "  \n",
        "  indices = list() # [(row_1, column_1), ..., (row_n, column_n)], donde n es igual a sum([len(bboxes) for bboxes in target_bboxes])\n",
        "\n",
        "  for batch_index in range(batch_size):\n",
        "    target_labels_of_batch = target_labels[batch_index]\n",
        "\n",
        "    target_indexes_of_batch = list(range(len(target_labels_of_batch)))\n",
        "\n",
        "    target_costs = total_cost[batch_index][:, target_indexes_of_batch]\n",
        "\n",
        "    target_costs = target_costs.cpu().detach().numpy()\n",
        "\n",
        "    #La función linear_sum_assignment encuentra las filas y columnas con los valores mínimos\n",
        "    rows, columns = linear_sum_assignment(target_costs)\n",
        "\n",
        "    rows = torch.as_tensor(rows, dtype=torch.int64)\n",
        "    columns = torch.as_tensor(columns, dtype=torch.int64)\n",
        "    \n",
        "    indices.append((rows, columns))\n",
        "  \n",
        "  return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8jd88LOu6rt"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Calcula la precisión para los valores especificados de k\"\"\"\n",
        "    if target.numel() == 0:\n",
        "        return [torch.zeros([], device=output.device)]\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\" Esta clase calcula el loss del DETR.\n",
        "    El proceso se da en dos etapas:\n",
        "        1) calculamos la asignación húngara entre los ground-truth boxes y los resultados del modelo\n",
        "        2) Supervisamos cada pair de ground-truth / prediction (supervise class and box)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, matcher, eos_coef=0.1, loss_ce=1, loss_bbox=5, loss_giou=2):\n",
        "        \"\"\" Creación del criterio.\n",
        "        Parámetros:\n",
        "            num_classes: Número de las categorías omitiendo la categoría vacía\n",
        "            matcher: módulo capaz de calcular una coincidencia entre objetivos y propuestas\n",
        "            eos_coef: peso de clasificación relativo aplicado a la categoría vacía\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.eos_coef = eos_coef\n",
        "        self.loss_ce = loss_ce\n",
        "        self.loss_bbox = loss_bbox\n",
        "        self.loss_giou = loss_giou\n",
        "\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.eos_coef\n",
        "\n",
        "        #Se agrega un búfer al módulo.\n",
        "\n",
        "        #Esto se usa normalmente para registrar un búfer que no debe considerarse un parámetro de modelo. \n",
        "        #Por ejemplo, running_mean de BatchNorm no es un parámetro, pero es parte del estado del módulo. \n",
        "        #Los búferes, de forma predeterminada, son persistentes y se guardarán junto con los parámetros. \n",
        "        #Este comportamiento se puede cambiar configurando persistent en Falso. \n",
        "        #La única diferencia entre un búfer persistente y un búfer no persistente es que este último\n",
        "        #no será parte del state_dict de este módulo.\n",
        "\n",
        "        #Se puede acceder a los búferes como atributos utilizando nombres dados.\n",
        "        self.register_buffer('empty_weight', empty_weight, persistent=True)\n",
        "\n",
        "    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        \"\"\"Loss de clasificación (NLL)\n",
        "        los dictados de objetivos deben contener las \"etiquetas\" clave que contienen un tensor de dim[nb_target_boxes]\n",
        "        \"\"\"\n",
        "        assert 'pred_logits' in outputs\n",
        "        src_logits = outputs['pred_logits']\n",
        "\n",
        "        # permutar predicciones siguiendo índices\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
        "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
        "                                    dtype=torch.int64, device=src_logits.device)\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
        "        losses = {'loss_ce': loss_ce}\n",
        "\n",
        "        if log == True:\n",
        "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
        "        return losses\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\" Calcula el error de cardinalidad, es decir, el error absoluto en el número de casillas no vacías pronosticadas\n",
        "        Esto no es realmente una pérdida, está destinado únicamente a fines de registro. No propaga gradientes.\n",
        "        \"\"\"\n",
        "        pred_logits = outputs['pred_logits']\n",
        "        device = pred_logits.device\n",
        "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
        "        # Cuenta el número de predicciones que NO son \"no-object\" (que es la última clase)\n",
        "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
        "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
        "        losses = {'cardinality_error': card_err}\n",
        "        return losses\n",
        "\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Calcula las pérdidas relacionadas con los cuadros delimitadores, la pérdida de regresión L1 y la pérdida de GIoU\n",
        "           los dictados de objetivos deben contener las \"cajas\" clave que contienen un tensor de dim[nb_target_boxes, 4]\n",
        "           Los boxes de destino se esperan en formato (center_x, center_y, w, h), normalizados por el tamaño de la imagen\n",
        "        \"\"\"\n",
        "        assert 'pred_bboxes' in outputs\n",
        "        # permutar predicciones siguiendo índices\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs['pred_bboxes'][idx]\n",
        "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "\n",
        "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
        "\n",
        "        losses = {}\n",
        "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
        "\n",
        "        loss_giou = 1 - torch.diag(generalized_box_iou(\n",
        "            bbox_cxcywh_to_xyxy(src_boxes),\n",
        "            bbox_cxcywh_to_xyxy(target_boxes)))\n",
        "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # Devuelve un tensor con el mismo tamaño que la entrada llena con fill_value\n",
        "        batch_idx = torch.cat([torch.full_like(input=src, fill_value=i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" Esto realiza el cálculo de pérdida.\n",
        "        Parámetros:\n",
        "             outputs: dict de tensores, consultar la especificación de salida del modelo para el formato\n",
        "             targets: lista de dict, tanto comp len(targets) == batch_size.\n",
        "                      Las claves esperadas en cada dict dependen de las pérdidas aplicadas, consultar el documento de cada pérdida\n",
        "        \"\"\"\n",
        "        prediction_logits = outputs[\"pred_logits\"]\n",
        "        prediction_bboxes = outputs[\"pred_bboxes\"]\n",
        "\n",
        "        target_labels = [target[\"labels\"] for target in targets]\n",
        "        target_bboxes = [target[\"boxes\"] for target in targets]\n",
        "\n",
        "        # Recuperar la coincidencia entre las salidas de la última capa y los objetivos\n",
        "        indices = self.matcher(prediction_logits, prediction_bboxes, target_labels, target_bboxes)\n",
        "\n",
        "        # Calcula el número promedio de boxes de destino en todos los nodos, con fines de normalización\n",
        "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
        "        num_boxes = torch.clamp(num_boxes, min=1).item()\n",
        "\n",
        "        # Calcula todos los losses\n",
        "        losses = dict()\n",
        "\n",
        "        losses.update(self.loss_labels(outputs, targets, indices, num_boxes))\n",
        "        losses.update(self.loss_cardinality(outputs, targets, indices, num_boxes))\n",
        "        losses.update(self.loss_boxes(outputs, targets, indices, num_boxes))\n",
        "\n",
        "        total_loss = (losses[\"loss_ce\"] * self.loss_ce) + (losses[\"loss_bbox\"] * self.loss_bbox) + (losses[\"loss_giou\"] * self.loss_giou)\n",
        "\n",
        "        return total_loss, losses[\"class_error\"], losses[\"cardinality_error\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qQ3gUcdytVJ"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, lr_scheduler, epochs, train_dataset_loader, test_dataset_loader, max_norm=0.1):\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "\n",
        "  train_accuracies = []\n",
        "  test_accuracies = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    train_loss = torch.tensor(0, dtype=torch.float32)\n",
        "    test_loss = torch.tensor(0, dtype=torch.float32)\n",
        "\n",
        "    train_accuracy = torch.tensor(0, dtype=torch.float32)\n",
        "    test_accuracy = torch.tensor(0, dtype=torch.float32)\n",
        "\n",
        "    ###################################################################################################################\n",
        "\n",
        "    #Establecer el modelo en modo entrenamiento\n",
        "    model.train()\n",
        "\n",
        "    #Establecer el criterion en modo entrenamiento\n",
        "    criterion.train()\n",
        "\n",
        "    #Número de batches\n",
        "    nb = len(train_dataset_loader)\n",
        "\n",
        "    #Número de registros\n",
        "    n_rows = len(train_dataset_loader.dataset)\n",
        "\n",
        "    for batch_x_train, batch_y_train in train_dataset_loader:\n",
        "      batch_x_train[\"images\"] = batch_x_train[\"images\"].to(device)\n",
        "      batch_x_train[\"masks\"] = batch_x_train[\"masks\"].to(device)\n",
        "\n",
        "      for targets in batch_y_train:\n",
        "        targets[\"labels\"] = targets[\"labels\"].to(device)\n",
        "        targets[\"boxes\"] = targets[\"boxes\"].to(device)\n",
        "\n",
        "      #PyTorch acumula gradientes, por eso necesitamos eliminarlas antes de cada instancia\n",
        "      model.zero_grad()\n",
        "\n",
        "      batch_train_predictions = model([batch_x_train[\"images\"], batch_x_train[\"masks\"]])\n",
        "\n",
        "      batch_train_loss, train_class_error, train_cardinality_error = criterion(batch_train_predictions, batch_y_train)\n",
        "\n",
        "      train_loss_value = batch_train_loss.item()\n",
        "      \n",
        "      if not math.isfinite(train_loss_value):\n",
        "        print(\"Batch Train Loss is {}, stopping training\".format(train_loss_value))\n",
        "        return\n",
        "      \n",
        "      #Sacamos la media para realizar el backward\n",
        "      batch_train_loss.backward()\n",
        "\n",
        "      if max_norm > 0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += batch_train_loss.item()\n",
        "\n",
        "      train_accuracy += (100 - train_class_error.item())\n",
        "    \n",
        "    lr_scheduler.step()\n",
        "\n",
        "    train_loss /= n_rows\n",
        "\n",
        "    #Necesitamos usar detach porque esta sección no se ha ejecutado con torch.no_grad()\n",
        "    #Eso implica que el tensor require grad\n",
        "    train_loss = train_loss.detach().numpy()\n",
        "    \n",
        "    train_accuracy /= nb\n",
        "    train_accuracy = train_accuracy.numpy()\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    ###################################################################################################################  \n",
        "    \n",
        "    #Establecer el modelo en modo prueba\n",
        "    model.eval()\n",
        "\n",
        "    #Establecer el criterion en modo prueba\n",
        "    criterion.eval()\n",
        "\n",
        "    #Número de batches\n",
        "    nb = len(test_dataset_loader)\n",
        "\n",
        "    #Número de registros\n",
        "    n_rows = len(test_dataset_loader.dataset)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for batch_x_test, batch_y_test in test_dataset_loader:\n",
        "        batch_x_test[\"images\"] = batch_x_test[\"images\"].to(device)\n",
        "        batch_x_test[\"masks\"] = batch_x_test[\"masks\"].to(device)\n",
        "\n",
        "        for targets in batch_y_test:\n",
        "          targets[\"labels\"] = targets[\"labels\"].to(device)\n",
        "          targets[\"boxes\"] = targets[\"boxes\"].to(device)\n",
        "\n",
        "        batch_test_predictions = model([batch_x_test[\"images\"], batch_x_test[\"masks\"]])\n",
        "\n",
        "        batch_test_loss, test_class_error, test_cardinality_error = criterion(batch_test_predictions, batch_y_test)\n",
        "        test_loss += batch_test_loss.item()\n",
        "\n",
        "        test_accuracy += (100 - test_class_error.item())\n",
        "      \n",
        "      test_loss /= n_rows\n",
        "      test_loss = test_loss.numpy()\n",
        "\n",
        "      test_accuracy /= nb\n",
        "      test_accuracy = test_accuracy.numpy()\n",
        "\n",
        "      test_losses.append(test_loss)\n",
        "      test_accuracies.append(test_accuracy)\n",
        "    \n",
        "    ###################################################################################################################\n",
        "    \n",
        "    print(\"Epoch {}/{}\\n----> loss: {:.4f} - accuracy: {:.4f} - val_loss: {:.4f} - val_accuracy: {:.4f}\".format(epoch+1, epochs, train_loss, train_accuracy, test_loss, test_accuracy))\n",
        "\n",
        "    model_file = \"my_model/model_epoch_{}.pt\".format(epoch+1)\n",
        "    optimizer_file = \"my_model/optimizer_epoch_{}.pt\".format(epoch+1)\n",
        "    lr_scheduler_file = \"my_model/lr_scheduler_epoch_{}.pt\".format(epoch+1)\n",
        "\n",
        "    save_weights(model, optimizer, lr_scheduler, model_file, optimizer_file, lr_scheduler_file)\n",
        "  \n",
        "  return train_losses, test_losses, train_accuracies, test_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "315Wu9jnkc0n"
      },
      "outputs": [],
      "source": [
        "def show_results(train_losses, test_losses, train_accuracies, test_accuracies):\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(14,4))\n",
        "  ax1, ax2 = axes\n",
        "  ax1.plot(train_losses, label='train')\n",
        "  ax1.plot(test_losses, label='test')\n",
        "  ax1.set_xlabel('epoch'); ax1.set_ylabel('loss')\n",
        "  ax2.plot(train_accuracies, label='train')\n",
        "  ax2.plot(test_accuracies, label='test')\n",
        "  ax2.set_xlabel('epoch'); ax2.set_ylabel('accuracy')\n",
        "  for ax in axes: ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p55EJ5rituzZ"
      },
      "outputs": [],
      "source": [
        "#Para el post-procesamiento de los bounding box de salida\n",
        "def bbox_cxcywh_to_xyxy(x):\n",
        "  #Convertir de x_c, y_c, w, h a x_0, y_0, x_1, y_1 para un bounding box (x_0, y_0), (x_0, y_1), (x_1, y_1), (x_1, y_0) donde x_c, y_c son x_center, y_center\n",
        "  x_c, y_c, w, h = x.unbind(1)\n",
        "  b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "       (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "  return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, img_w, img_h):\n",
        "  b = bbox_cxcywh_to_xyxy(out_bbox)\n",
        "  b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "  return b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fr26zOttuzZ"
      },
      "outputs": [],
      "source": [
        "def predict(model, images, device, threshold=0.7):\n",
        "  images_width = [image.width for image in images]\n",
        "  images_height = [image.height for image in images]\n",
        "\n",
        "  return_masks = True\n",
        "  \n",
        "  mode = \"eval\"\n",
        "\n",
        "  if return_masks == False:\n",
        "    images = process_images(images, mode=mode, device=device, return_masks=return_masks)\n",
        "  else:\n",
        "    images, masks = process_images(images, mode=mode, device=device, return_masks=return_masks)\n",
        "\n",
        "  #Propagar a través del modelo\n",
        "  if return_masks == False:\n",
        "    outputs = model(images)\n",
        "  else:\n",
        "    outputs = model([images, masks])\n",
        "\n",
        "  #Mantener solo donde predictions >= threshold\n",
        "  images_probabilities = outputs[\"pred_logits\"].softmax(-1)\n",
        "  \n",
        "  images_bboxes = outputs[\"pred_bboxes\"]\n",
        "\n",
        "  images_probabilities_and_bboxes = list()\n",
        "  \n",
        "  for image_probabilities, image_bboxes, image_width, image_height in zip(images_probabilities, images_bboxes, images_width, images_height):\n",
        "    #Remover las clases extras\n",
        "    image_probabilities = image_probabilities[:, :-1]\n",
        "\n",
        "    keep = image_probabilities.max(-1).values > threshold\n",
        "\n",
        "    image_probabilities = image_probabilities[keep].cpu()\n",
        "    image_bboxes = image_bboxes[keep].cpu()\n",
        "\n",
        "    #Convertir bounding boxes del rango [0, 1] a la escala de imagen original\n",
        "    image_bboxes = rescale_bboxes(image_bboxes, image_width, image_height)\n",
        "\n",
        "    images_probabilities_and_bboxes.append([image_probabilities, image_bboxes])\n",
        "  \n",
        "  return images_probabilities_and_bboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2eliSpJtuzZ"
      },
      "outputs": [],
      "source": [
        "def plot_results(pil_img, prob, bboxes, classes, colors):\n",
        "  plt.figure(figsize=(16,10))\n",
        "  plt.imshow(pil_img)\n",
        "  \n",
        "  if prob.shape[0] != 0:\n",
        "    ax = plt.gca()\n",
        "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, bboxes.tolist(), colors * 100):\n",
        "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                    fill=False, color=c, linewidth=3))\n",
        "        cl = p.argmax()\n",
        "        text = f\"{classes[cl]}: {p[cl]:0.2f}\"\n",
        "        print(text)\n",
        "        ax.text(xmin, ymin, text, fontsize=15,\n",
        "                bbox=dict(facecolor=\"yellow\", alpha=0.5))\n",
        "  else:\n",
        "    print(\"There is no objects\")\n",
        "  \n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0X1V1-b8Num"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orpZHEEgSPSP"
      },
      "outputs": [],
      "source": [
        "%mkdir -p my_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_pMRYUGePSy"
      },
      "outputs": [],
      "source": [
        "images_folder_path = \"ImagesT\"\n",
        "annotations_folder_path = \"AnnotationsT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UHSDWeYFa_W"
      },
      "outputs": [],
      "source": [
        "images_filenames = list()\n",
        "annotations_filenames = list()\n",
        "\n",
        "for file_name in listdir(images_folder_path):\n",
        "  image_path = images_folder_path + \"/\" + file_name\n",
        "  annotation_path = annotations_folder_path + \"/\" + file_name.rsplit(\".\", 1)[0] + \".json\"\n",
        "\n",
        "  if (exists(image_path) == True) and (exists(annotation_path) == True):\n",
        "    images_filenames.append(image_path)\n",
        "    annotations_filenames.append(annotation_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd4rp0OSWy4O"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(images_filenames, annotations_filenames, test_size=0.15, random_state=0)\n",
        "\n",
        "len(x_train), len(y_train), len(x_test), len(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GFea2nZXAKm"
      },
      "outputs": [],
      "source": [
        "#Modificar el valor para mejorar el entrenamiento\n",
        "#batch_size = 1\n",
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "devXO987IQaP"
      },
      "outputs": [],
      "source": [
        "train_transformations = get_transformation(mode=\"train\")\n",
        "\n",
        "train_data = ApplyTransformationToDataset(x_train, y_train, train_transformations)\n",
        "\n",
        "train_dataset_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, collate_fn=padding_sample_batch, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY55dlQJIOPh"
      },
      "outputs": [],
      "source": [
        "test_transformations = get_transformation(mode=\"eval\")\n",
        "\n",
        "test_data = ApplyTransformationToDataset(x_test, y_test, test_transformations)\n",
        "\n",
        "test_dataset_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, collate_fn=padding_sample_batch, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL0fW2m7w0LD"
      },
      "outputs": [],
      "source": [
        "criterion = SetCriterion(num_classes, matching_function)\n",
        "\n",
        "criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAD21y-kannQ"
      },
      "outputs": [],
      "source": [
        "param_dicts = [\n",
        "                {\"params\": [parameter for name, parameter in model.named_parameters() if (\"backbone\" not in name) and (parameter.requires_grad == True)]},\n",
        "                {\n",
        "                    \"params\": [parameter for name, parameter in model.named_parameters() if (\"backbone\" in name) and (parameter.requires_grad == True)],\n",
        "                    \"lr\": 0.00001,\n",
        "                },\n",
        "              ]\n",
        "\n",
        "optimizer = torch.optim.AdamW(param_dicts, lr=0.0001, weight_decay=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPqEISF4JmyH"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpaaOfF-N70m"
      },
      "outputs": [],
      "source": [
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR54kS9cLX95"
      },
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "\n",
        "model_file = \"best_my_model/model_epoch_17.pt\"\n",
        "optimizer_file = \"best_my_model/optimizer_epoch_17.pt\"\n",
        "lr_scheduler_file = \"best_my_model/lr_scheduler_epoch_17.pt\"\n",
        "\n",
        "model, optimizer, lr_scheduler = reset_weights(model, optimizer, lr_scheduler, model_file, optimizer_file, lr_scheduler_file)\n",
        "model.eval();\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "train_losses, test_losses, train_accuracies, test_accuracies = train_model(model, criterion, optimizer, lr_scheduler, epochs, train_dataset_loader, test_dataset_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkPpQpC3Lauy"
      },
      "outputs": [],
      "source": [
        "show_results(train_losses, test_losses, train_accuracies, test_accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZLnMb-G8KpV"
      },
      "source": [
        "## Model Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ-VyVMEIw3g"
      },
      "outputs": [],
      "source": [
        "#Modificar dependiendo del dataset\n",
        "classes = ['vehicles', 'Ambulance', 'Bus', 'Car', 'Motorcycle', 'Truck']\n",
        "\n",
        "#Colores para la visualización\n",
        "colors = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg7T7iobbAmS"
      },
      "outputs": [],
      "source": [
        "#Modificar para usar la mejor época\n",
        "model_file = \"my_model/model_epoch_26.pt\"\n",
        "optimizer_file = \"my_model/optimizer_epoch_26.pt\"\n",
        "lr_scheduler_file = \"my_model/lr_scheduler_epoch_26.pt\"\n",
        "\n",
        "model, optimizer, lr_scheduler = reset_weights(model, optimizer, lr_scheduler, model_file, optimizer_file, lr_scheduler_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32Ztpzdmaxb9"
      },
      "outputs": [],
      "source": [
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L3KTIqpIw6K"
      },
      "outputs": [],
      "source": [
        "#image_path = \"images1/00aaf0a0a9ee7e71_jpg.rf.808b1e59067887493dffad63561c2a9d.jpg\"\n",
        "image_path = \"ImagesT/00aaf0a0a9ee7e71_jpg.rf.808b1e59067887493dffad63561c2a9d.jpg\"\n",
        "\n",
        "image = Image.open(image_path)\n",
        "\n",
        "images_probabilities_and_bboxes = predict(model, [image], device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l1HYPYTKHor"
      },
      "outputs": [],
      "source": [
        "scores, boxes = images_probabilities_and_bboxes[0]\n",
        "\n",
        "plot_results(image, scores, boxes, classes, colors)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ProyectoFinal_Cruyffismo.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "77464af022044613bec8498f5cfd389b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_daab1cd759014010ba69852f739eaa44",
              "IPY_MODEL_326a4b882cf74509bcc2e7a389aa614b",
              "IPY_MODEL_624858025d6544a7b8b2b9bd9e952e99"
            ],
            "layout": "IPY_MODEL_d44392fee29a40a6a065f952192adc21"
          }
        },
        "daab1cd759014010ba69852f739eaa44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e507fe71f9d54a2c8879a162c62a419b",
            "placeholder": "​",
            "style": "IPY_MODEL_58e903873085427c8673513983603787",
            "value": "100%"
          }
        },
        "326a4b882cf74509bcc2e7a389aa614b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e7263e47ec74a4783ad90fb4203fc88",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7d16c291a3541928c2c1d275416d526",
            "value": 102530333
          }
        },
        "624858025d6544a7b8b2b9bd9e952e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65f8eda2fd2e4a9cb55c9b5b629f9df4",
            "placeholder": "​",
            "style": "IPY_MODEL_a6f9d60c3e834077a53f6266c2caa4a6",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 197MB/s]"
          }
        },
        "d44392fee29a40a6a065f952192adc21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e507fe71f9d54a2c8879a162c62a419b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e903873085427c8673513983603787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e7263e47ec74a4783ad90fb4203fc88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7d16c291a3541928c2c1d275416d526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65f8eda2fd2e4a9cb55c9b5b629f9df4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6f9d60c3e834077a53f6266c2caa4a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}